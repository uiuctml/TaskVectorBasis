{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/cindy2000_sh/TsakVectorBasis')\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "from src.task_vectors import NonLinearTaskVector, LinearizedTaskVector\n",
    "import numpy as np\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_task_vector(model, task, pretrained_ckpt_name, finetuning_mode):\n",
    "    if finetuning_mode == 'standard':\n",
    "        pretrained_checkpoint = f'/home/cindy2000_sh/ntk-llm/tangent_task_arithmetic/checkpoints_{pretrained_ckpt_name}/{model}/zeroshot.pt'\n",
    "        finetuned_checkpoint = f'/home/cindy2000_sh/ntk-llm/tangent_task_arithmetic/checkpoints_{pretrained_ckpt_name}/{model}/{task}Val/finetuned.pt'\n",
    "        return NonLinearTaskVector(pretrained_checkpoint, finetuned_checkpoint)\n",
    "    else:\n",
    "        pretrained_checkpoint = f'/home/cindy2000_sh/ntk-llm/tangent_task_arithmetic/checkpoints_{pretrained_ckpt_name}/{model}/{task}Val/linear_zeroshot.pt'\n",
    "        finetuned_checkpoint = f'/home/cindy2000_sh/ntk-llm/tangent_task_arithmetic/checkpoints_{pretrained_ckpt_name}/{model}/{task}Val/linear_finetuned.pt'\n",
    "        return LinearizedTaskVector(pretrained_checkpoint, finetuned_checkpoint)\n",
    "\n",
    "def flatten_task_vector(task_vector):\n",
    "    flattened_vector = []\n",
    "    param_keys = []\n",
    "\n",
    "    for key, tensor in task_vector.vector.items():\n",
    "        flattened_vector.append(tensor.cpu().numpy().ravel())  \n",
    "        param_keys.append(key)\n",
    "\n",
    "    flat_vector = np.concatenate(flattened_vector)\n",
    "    return flat_vector, param_keys\n",
    "\n",
    "def load_and_flatten_all_task_vectors(model, task_names, pretrained_ckpt_name, finetuning_mode):\n",
    "    flattened_vectors = []\n",
    "    all_param_keys = None  \n",
    "\n",
    "    for task in task_names:\n",
    "        task_vector = load_task_vector(model, task, pretrained_ckpt_name, finetuning_mode)\n",
    "        flat_vector, param_keys = flatten_task_vector(task_vector)\n",
    "\n",
    "        if all_param_keys is None:\n",
    "            all_param_keys = param_keys \n",
    "\n",
    "        flattened_vectors.append(flat_vector)\n",
    "\n",
    "    return np.vstack(flattened_vectors), all_param_keys\n",
    "\n",
    "\n",
    "def recover_task_vector_from_centroid(centroid, param_keys, task_vector_template):\n",
    "    recovered_vector = {}\n",
    "    start = 0\n",
    "\n",
    "    for key in param_keys:\n",
    "        original_shape = task_vector_template.vector[key].shape\n",
    "\n",
    "        num_elements = int(np.prod(original_shape))\n",
    "\n",
    "        recovered_vector[key] = torch.tensor(centroid[start:start + num_elements]).reshape(original_shape)\n",
    "\n",
    "        start += num_elements\n",
    "\n",
    "    return recovered_vector\n",
    "\n",
    "def save_recovered_task_vector_as_checkpoint(recovered_vector, model_checkpoint_path):\n",
    "    torch.save(recovered_vector, model_checkpoint_path)\n",
    "\n",
    "\n",
    "def save_all_centroids_as_checkpoints(centroids, param_keys, task_vector_template, output_dir):\n",
    "    import os\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for idx, centroid in enumerate(centroids):\n",
    "        recovered_vector = recover_task_vector_from_centroid(centroid, param_keys, task_vector_template)\n",
    "\n",
    "        checkpoint_path = os.path.join(output_dir, f'centroid_{idx}.pt')\n",
    "\n",
    "        save_recovered_task_vector_as_checkpoint(recovered_vector, checkpoint_path)\n",
    "        print(f\"Saved centroid {idx} as {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_5_percent_masked_centroid(cluster_points):\n",
    "    if not isinstance(cluster_points, torch.Tensor):\n",
    "        cluster_points = torch.tensor(cluster_points, dtype=torch.float32)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        cluster_points = cluster_points.to(device='cuda:2')\n",
    "\n",
    "    n, d = cluster_points.shape\n",
    "    top_5_percent = int(torch.ceil(torch.tensor(d * 0.05)))\n",
    "    abs_values = torch.abs(cluster_points)\n",
    "    thresholds, _ = torch.kthvalue(abs_values, k=d - top_5_percent + 1, dim=1)\n",
    "    mask = abs_values >= thresholds.unsqueeze(1)\n",
    "    normalized_mask = mask.float() / n\n",
    "    masked_rows = normalized_mask * cluster_points\n",
    "    centroid = masked_rows.sum(dim=0)\n",
    "\n",
    "    del cluster_points, abs_values, thresholds, mask, normalized_mask, masked_rows\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_values_mask(M, K, return_mask=False):\n",
    "    if K > 1:\n",
    "        K /= 100\n",
    "\n",
    "    original_shape = M.shape\n",
    "    if M.dim() == 1:\n",
    "        M = M.unsqueeze(0)\n",
    "\n",
    "    n, d = M.shape\n",
    "    k = int(d * K)\n",
    "    k = d - k  \n",
    "\n",
    "    _, indices = M.abs().topk(k, dim=1, largest=True, sorted=False)\n",
    "    mask = torch.zeros_like(M, dtype=torch.bool).scatter_(1, indices, True)\n",
    "    final_mask = mask.squeeze() if original_shape == M.squeeze().shape else mask\n",
    "\n",
    "    if return_mask:\n",
    "        return M * final_mask, final_mask.float().mean(dim=1), final_mask\n",
    "    return M * final_mask, final_mask.float().mean(dim=1)\n",
    "\n",
    "def resolve_zero_signs(sign_to_mult, method=\"majority\"):\n",
    "    majority_sign = torch.sign(sign_to_mult.sum())\n",
    "\n",
    "    if method == \"majority\":\n",
    "        sign_to_mult[sign_to_mult == 0] = majority_sign\n",
    "    elif method == \"minority\":\n",
    "        sign_to_mult[sign_to_mult == 0] = -1 * majority_sign\n",
    "    return sign_to_mult\n",
    "\n",
    "def resolve_sign(Tensor):\n",
    "    sign_to_mult = torch.sign(Tensor.sum(dim=0))\n",
    "    sign_to_mult = resolve_zero_signs(sign_to_mult, \"majority\")\n",
    "    return sign_to_mult\n",
    "\n",
    "def disjoint_merge(Tensor, merge_func, sign_to_mult):\n",
    "    merge_func = merge_func.split(\"-\")[-1]\n",
    "\n",
    "    if sign_to_mult is not None:\n",
    "        rows_to_keep = torch.where(\n",
    "            sign_to_mult.unsqueeze(0) > 0, Tensor > 0, Tensor < 0\n",
    "        )\n",
    "        selected_entries = Tensor * rows_to_keep\n",
    "    else:\n",
    "        rows_to_keep = Tensor != 0\n",
    "        selected_entries = Tensor * rows_to_keep\n",
    "\n",
    "    if merge_func == \"mean\":\n",
    "        non_zero_counts = (selected_entries != 0).sum(dim=0).float()\n",
    "        disjoint_aggs = torch.sum(selected_entries, dim=0) / torch.clamp(\n",
    "            non_zero_counts, min=1\n",
    "        )\n",
    "    elif merge_func == \"sum\":\n",
    "        disjoint_aggs = torch.sum(selected_entries, dim=0)\n",
    "    elif merge_func == \"max\":\n",
    "        disjoint_aggs = selected_entries.abs().max(dim=0)[0]\n",
    "        disjoint_aggs *= sign_to_mult\n",
    "    else:\n",
    "        raise ValueError(f\"Merge method {merge_func} is not defined.\")\n",
    "\n",
    "    return disjoint_aggs\n",
    "\n",
    "def ties_merging(flat_task_checks, reset_thresh=None, merge_func=\"dis-mean\"):\n",
    "    all_checks = flat_task_checks.clone()\n",
    "    updated_checks, *_ = topk_values_mask(\n",
    "        all_checks, K=reset_thresh, return_mask=False\n",
    "    )\n",
    "    final_signs = resolve_sign(updated_checks)\n",
    "    merged_tv = disjoint_merge(updated_checks, merge_func, final_signs)\n",
    "    return merged_tv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fisher_for_dataset(fisher_dir, dataset_name):\n",
    "    def flatten_fisher_matrix(fisher):\n",
    "        return torch.cat([v.flatten() for v in fisher.values()])\n",
    "    fisher_path = f\"{fisher_dir}/{dataset_name}Val/fisher_train.pth\"\n",
    "    fisher = torch.load(fisher_path)  \n",
    "    return flatten_fisher_matrix(fisher)\n",
    "\n",
    "def compute_fisher_weighted_centroid(cluster_points, fisher_matrices, lamb):\n",
    "    \"\"\"\n",
    "    Computes the Fisher-weighted centroid using the formula:\n",
    "    Merged Model = (sum_i (lambda * Fisher_i * Task_Vector_i)) / (sum_i (lambda * Fisher_i))\n",
    "    \"\"\"\n",
    "    fisher_weighted_sum = None\n",
    "    fisher_sum = None\n",
    "\n",
    "    for task_vector, fisher in zip(cluster_points, fisher_matrices):\n",
    "        task_vector_tensor = torch.tensor(task_vector, dtype=torch.float32)\n",
    "        fisher_tensor = fisher.to(dtype=torch.float32, device=task_vector_tensor.device)\n",
    "        weighted_task_vector = lamb * fisher_tensor * task_vector_tensor\n",
    "        fisher_weighted_sum = (\n",
    "            weighted_task_vector if fisher_weighted_sum is None else fisher_weighted_sum + weighted_task_vector\n",
    "        )\n",
    "\n",
    "        fisher_sum = (\n",
    "            lamb * fisher_tensor if fisher_sum is None else fisher_sum + lamb * fisher_tensor\n",
    "        )\n",
    "\n",
    "    centroid = fisher_weighted_sum / fisher_sum\n",
    "    centroid[torch.isnan(centroid)] = 0 \n",
    "    return centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inner Merge (Here we evaluate on all partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_centroids(flattened_matrix, labels, n_clusters, mean='euclidean', reset_thresh=20, merge_func=\"dis-mean\"):\n",
    "    centroids = []\n",
    "    fisher_dir = \"/home/cindy2000_sh/ntk-llm/tangent_task_arithmetic/checkpoints_laion2b_e16/ViT-B-32\"\n",
    "\n",
    "    for cluster in range(n_clusters):\n",
    "        cluster_points = flattened_matrix[labels == cluster]\n",
    "        dataset_names = np.array(task_names)[labels == cluster] \n",
    "\n",
    "        if len(cluster_points) == 0:\n",
    "            continue \n",
    "        if len(cluster_points) == 1:\n",
    "            centroids.append(np.mean(cluster_points, axis=0))\n",
    "            continue \n",
    "\n",
    "        cluster_points_tensor = torch.tensor(cluster_points, dtype=torch.float32).to('cuda:2')\n",
    "\n",
    "        if mean == 'mean' or 'tangent':\n",
    "            centroid = np.mean(cluster_points, axis=0)\n",
    "\n",
    "        elif mean == 'ties':\n",
    "            centroid = ties_merging(\n",
    "                flat_task_checks=cluster_points_tensor, \n",
    "                reset_thresh=reset_thresh,\n",
    "                merge_func=merge_func\n",
    "            ).numpy()  \n",
    "\n",
    "        elif mean == 'fisher':\n",
    "            fisher_matrices = [\n",
    "                compute_fisher_for_dataset(fisher_dir, dataset) for dataset in dataset_names\n",
    "            ]\n",
    "            lamb = 1 / len(cluster_points)  # Equal weighting for all datasets in the cluster\n",
    "            centroid = compute_fisher_weighted_centroid(cluster_points, fisher_matrices, lamb).numpy()\n",
    "\n",
    "        centroids.append(centroid)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = ['ties','mean','topk']\n",
    "finetuning_mode = 'standard'\n",
    "task_names = ['MNIST', 'EuroSAT', 'RESISC45', 'SVHN']\n",
    "model_names = ['ViT-L-14']\n",
    "pretrained_ckpt_names = ['openai']\n",
    "\n",
    "label_comb = [[1,0,1,1],\n",
    "              [2,0,1,2],\n",
    "              [2,0,2,1],\n",
    "              [0,1,2,2],\n",
    "              [0,2,1,2],\n",
    "              [0,2,3,1],\n",
    "              [0,2,2,1],\n",
    "              [0,1,1,1],\n",
    "              [0,0,1,1],\n",
    "              [0,1,0,1],\n",
    "              [0,1,1,0],\n",
    "              [0,0,0,0],\n",
    "              [1,1,0,1],\n",
    "              [1,1,1,0],\n",
    "              [2,2,1,0]]\n",
    "\n",
    "flattened_matrix, param_keys = load_and_flatten_all_task_vectors(model_names[0], task_names, pretrained_ckpt_names[0], finetuning_mode)\n",
    "task_vector_template = load_task_vector(model_names[0], task_names[0], pretrained_ckpt_names[0], finetuning_mode)\n",
    "\n",
    "for mean in means:\n",
    "    for labels in label_comb:\n",
    "        labels = np.array(labels)\n",
    "        parts = [\"\"] * len(task_names)\n",
    "        for task, lbl in zip(task_names, labels):\n",
    "            parts[lbl] += task[0] \n",
    "        folder_name = '-'.join(filter(None, parts))\n",
    "        output_dir = f'tangent_task_arithmetic/checkpoints_{pretrained_ckpt_names[0]}/{model_names[0]}/MERS-{mean}/{folder_name}_checkpoints/' # create folder\n",
    "        Path(output_dir).mkdir(parents=True)\n",
    "        task_to_label_mapping = dict(zip(task_names, [int(l) for l in labels]))\n",
    "        with open(output_dir + 'task_to_label_mapping.json', 'w') as f:\n",
    "            json.dump(task_to_label_mapping, f)\n",
    "        centroids = compute_cluster_centroids(flattened_matrix, labels, len(np.unique(labels)), mean)\n",
    "        save_all_centroids_as_checkpoints(centroids, param_keys, task_vector_template, output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outer Merge (Mean/Topk/TIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "def flatten_state_dict(state_dict):\n",
    "    flattened = []\n",
    "    param_keys = []\n",
    "    for key, param in state_dict.items():\n",
    "        flattened.append(param.view(-1))  \n",
    "        param_keys.append(key)\n",
    "    return torch.cat(flattened), param_keys\n",
    "\n",
    "def reconstruct_state_dict(flattened, param_keys, original_state_dict):\n",
    "    reconstructed = {}\n",
    "    offset = 0\n",
    "    for key in param_keys:\n",
    "        original_param = original_state_dict[key]\n",
    "        numel = original_param.numel()\n",
    "        reconstructed[key] = flattened[offset:offset+numel].view_as(original_param)\n",
    "        offset += numel\n",
    "    return reconstructed\n",
    "\n",
    "# Function to outer merge centroids using different methods\n",
    "def merge_centroids(flattened_centroids, method, top_k=5, reset_thresh=20, merge_func=\"dis-mean\"):\n",
    "    centroids_tensor = torch.stack(flattened_centroids, dim=0)\n",
    "\n",
    "    if method == \"mean\":\n",
    "        return torch.mean(centroids_tensor, dim=0)\n",
    "    elif method == \"topk\":\n",
    "        return top_5_percent_masked_centroid(centroids_tensor)\n",
    "    elif method == \"ties\":\n",
    "        return ties_merging(centroids_tensor, reset_thresh=reset_thresh, merge_func=merge_func)\n",
    "    else:\n",
    "        # for remaining method like TA, don't use the notebook\n",
    "        raise ValueError(f\"Unsupported merge method: {method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method in ['mean','ties','topk']:\n",
    "    base_dir = f'/data/common/cindy2000_sh/tangent_task_arithmetic/checkpoints/ViT-B-16/MERS-{method}'\n",
    "\n",
    "    for split_folder in os.listdir(base_dir):\n",
    "        split_path = os.path.join(base_dir, split_folder)\n",
    "        if not os.path.isdir(split_path):\n",
    "            continue \n",
    "\n",
    "        flattened_centroids = []\n",
    "        param_keys = None\n",
    "        example_state_dict = None\n",
    "\n",
    "        for file_name in os.listdir(split_path):\n",
    "            if file_name.startswith(\"centroid_\") and file_name.endswith(\".pt\"):\n",
    "                centroid_path = os.path.join(split_path, file_name)\n",
    "                state_dict = torch.load(centroid_path)\n",
    "                flattened, keys = flatten_state_dict(state_dict)\n",
    "                flattened_centroids.append(flattened)\n",
    "                if param_keys is None:\n",
    "                    param_keys = keys\n",
    "                    example_state_dict = state_dict\n",
    "\n",
    "        if not flattened_centroids:\n",
    "            continue  \n",
    "\n",
    "        merged_flattened_centroid = merge_centroids(flattened_centroids, method)\n",
    "\n",
    "        merged_state_dict = reconstruct_state_dict(merged_flattened_centroid, param_keys, example_state_dict)\n",
    "\n",
    "        output_file = os.path.join(split_path, f\"merged_centroid_{method}.pt\")\n",
    "        torch.save(merged_state_dict, output_file)\n",
    "        print(f\"Merged centroid saved for {split_folder} using {method}: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when outer merge = TA\n",
    "# run task vector addition outside\n",
    "# see cmd/run_outerTA.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tangent-arithmetic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
